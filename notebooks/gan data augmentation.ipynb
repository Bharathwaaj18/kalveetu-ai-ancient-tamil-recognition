{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image, make_grid\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndata_path = \"/kaggle/input/tamil_letters_augmented\"\noutput_dir = \"/kaggle/working/generated_letters\"\nos.makedirs(output_dir, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 64\nimage_size = 64  \nnz = 100         \nnum_epochs = 50\nlr = 0.0002\nbeta1 = 0.5","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.CenterCrop(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\ndataset = datasets.ImageFolder(root=data_path, transform=transform)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\nnum_classes = len(dataset.classes)\nprint(f\"{len(dataset)} images, {num_classes} classes found\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, nz, ngf=64, nc=3):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            # input Z, going into a convolution\n            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf*8),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf*4),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf*2),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()  # output [-1,1]\n        )\n\n    def forward(self, input):\n        return self.main(input)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, nc=3, ndf=64):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf*2),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf*4),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf*4, ndf*8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf*8),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf*8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()  # output probability\n        )\n\n    def forward(self, input):\n        return self.main(input).view(-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"netG = Generator(nz).to(device)\nnetD = Discriminator().to(device)\n\ncriterion = nn.BCELoss()\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\nfor epoch in range(num_epochs):\n    for i, (imgs, _) in enumerate(dataloader):\n        imgs = imgs.to(device)\n        b_size = imgs.size(0)\n        real_labels = torch.ones(b_size, device=device)\n        fake_labels = torch.zeros(b_size, device=device)\n\n        netD.zero_grad()\n        outputs = netD(imgs)\n        d_loss_real = criterion(outputs, real_labels)\n        d_loss_real.backward()\n\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        fake_imgs = netG(noise)\n        outputs = netD(fake_imgs.detach())\n        d_loss_fake = criterion(outputs, fake_labels)\n        d_loss_fake.backward()\n        optimizerD.step()\n        d_loss = d_loss_real + d_loss_fake\n\n        netG.zero_grad()\n        outputs = netD(fake_imgs)\n        g_loss = criterion(outputs, real_labels)\n        g_loss.backward()\n        optimizerG.step()\n\n        if i % 50 == 0:\n            print(f\"[{epoch}/{num_epochs}][{i}/{len(dataloader)}] D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n        fake = netG(fixed_noise).detach().cpu()\n    grid = make_grid(fake, normalize=True)\n    save_image(grid, f\"{output_dir}/epoch_{epoch}.png\")\n\nprint(\"GAN training complete! Generated images saved in\", output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}